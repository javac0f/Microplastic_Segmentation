{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jcof2\\anaconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# DS LIBS\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# VISUALIZATION LIBS\n",
    "import cv2\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SYSTEM\n",
    "import os\n",
    "import config\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# DL LIBS\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import relu\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "\n",
    "from segmentation_models_pytorch.utils.train import TrainEpoch, ValidEpoch\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import albumentations as album"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_list:str, mask_list:str, augmentation = None, preprocessing = None):\n",
    "        self.image_list = image_list\n",
    "        self.mask_list = mask_list\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # GET ITEM\n",
    "        image = cv2.resize(self.image_list[index], (config.IMAGE_SIZE , config.IMAGE_SIZE))\n",
    "        #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        # GET MASK \n",
    "        mask = cv2.resize(self.mask_list[index], (config.IMAGE_SIZE , config.IMAGE_SIZE))\n",
    "        #mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n",
    "        mask = self.transform(mask)\n",
    "\n",
    "        # apply augmentations\n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        # apply preprocessing\n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "\n",
    "    def transform(self, image):\n",
    "        img_mean, img_std = np.mean(image, axis=(0, 1)), np.std(image, axis=(0, 1))\n",
    "\n",
    "        tensor_transforms = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=img_mean,std=img_std)\n",
    "        ])\n",
    "        \n",
    "        return tensor_transforms(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET DATA\n",
    "x:list = []\n",
    "y:list = []\n",
    "\n",
    "# GET X DATA\n",
    "for filename in os.listdir(config.X_TRAIN_DATA_TOY):\n",
    "    full_path = os.path.join(config.X_TRAIN_DATA, filename)\n",
    "    image = cv2.imread(full_path)\n",
    "    x.append(image)\n",
    "\n",
    "# GET Y DATA\n",
    "for filename in os.listdir(config.Y_TRAIN_DATA_TOY):\n",
    "    full_path = os.path.join(config.Y_TRAIN_DATA, filename)\n",
    "    mask = cv2.imread(full_path)\n",
    "    y.append(mask)\n",
    "\n",
    "\n",
    "# SPLIT DATA INTO TRAIN AND VAL\n",
    "x_train, x_val, y_train, y_val = train_test_split(x,y,test_size=config.TEST_SPLIT, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x0000020550274210>\n"
     ]
    }
   ],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = ImageDataset(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    #augmentation=get_training_augmentation(),\n",
    "    #preprocessing=get_preprocessing(preprocessing_fn=None),\n",
    ")\n",
    "\n",
    "valid_dataset = ImageDataset(\n",
    "    x_val,\n",
    "    y_val, \n",
    "    #augmentation=get_validation_augmentation(), \n",
    "    #preprocessing=get_preprocessing(preprocessing_fn=None),\n",
    ")\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=12)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = TrainEpoch(\n",
    "    config.MODEL, \n",
    "    loss = config.LOSS, \n",
    "    metrics = config.METRICS, \n",
    "    optimizer = config.OPTIMIZER,\n",
    "    device = config.DEVICE,\n",
    "    verbose = True,\n",
    ")\n",
    "\n",
    "valid_epoch = ValidEpoch(\n",
    "    config.MODEL, \n",
    "    loss = config.LOSS, \n",
    "    metrics = config.METRICS, \n",
    "    device = config.DEVICE,\n",
    "    verbose = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0\n",
      "train:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if(config.TRAINING == True):\n",
    "\n",
    "    best_iou = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for index in range(0, config.EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(index))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou = valid_logs['iou_score']\n",
    "            torch.save(model, './best_model.pth at iteration {}'.format(index))\n",
    "            print('Model saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
